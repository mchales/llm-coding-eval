# llm-coding-eval

Exploring error driven prompting techniques on HumanEval and TBD code evaluation datasets.
